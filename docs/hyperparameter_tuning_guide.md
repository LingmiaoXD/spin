# SPIN模型超参数调整指南

当模型效果不如均值baseline时，可以尝试调整以下参数：

## 🔴 高优先级参数（最可能影响效果）

### 1. **模型容量参数**
- **`hidden_size`** (当前: 16) ⚠️ **可能太小**
  - 建议值: `32`, `64`, `128`
  - 说明: 当前16可能容量不足，建议先尝试32或64
  - 影响: 模型表达能力，过小会导致欠拟合

- **`n_layers`** (当前: 3)
  - 建议值: `4`, `5`
  - 说明: 增加层数可以提升模型深度，但要注意过拟合
  - 影响: 模型深度和特征提取能力

### 2. **学习率相关**
- **`lr`** (当前: 0.0008)
  - 建议值: `0.001`, `0.0015`, `0.002`
  - 说明: 如果模型收敛慢或效果差，可以尝试增大学习率
  - 影响: 训练速度和收敛性

- **`lr_scheduler`** (当前: magic)
  - 可选值: `cosine`, `magic`, `None`
  - 说明: cosine可能更稳定，magic有重启机制
  - 影响: 学习率衰减策略

### 3. **训练策略参数**
- **`whiten_prob`** (当前: [0.2, 0.5, 0.8])
  - 建议值: `[0.1, 0.3, 0.5]` 或 `0.2` (单个值)
  - 说明: 控制训练时注入缺失值的概率，过高可能导致训练困难
  - 影响: 训练时的缺失值模拟，影响模型学习难度

- **`epochs`** (当前: 80)
  - 建议值: `150`, `200`, `300`
  - 说明: 当前80可能训练不充分，特别是hidden_size增大后
  - 影响: 训练轮数，需要与patience配合

- **`patience`** (当前: 5) ⚠️ **可能太小**
  - 建议值: `20`, `30`, `40`
  - 说明: 5轮早停可能过早，建议至少20-30
  - 影响: 早停机制，过小会提前停止训练

## 🟡 中优先级参数

### 4. **损失函数和正则化**
- **`loss_fn`** (当前: l1_loss)
  - 可选值: `l1_loss`, `mse_loss` (l2_loss)
  - 说明: L1对异常值更鲁棒，L2可能在某些数据上效果更好
  - 影响: 损失计算方式

- **`l2_reg`** (当前: 0.0)
  - 建议值: `1e-5`, `1e-4`, `1e-3`
  - 说明: 添加L2正则化防止过拟合
  - 影响: 模型正则化

- **`grad_clip_val`** (当前: 5.0)
  - 建议值: `1.0`, `2.0`, `5.0`, `10.0`
  - 说明: 梯度裁剪，防止梯度爆炸
  - 影响: 训练稳定性

### 5. **模型结构参数**
- **`eta`** (当前: 2)
  - 建议值: `2`, `3`
  - 说明: 控制mask embedding引入的层，影响模型对缺失值的处理
  - 影响: 模型对缺失值的区分能力

- **`message_layers`** (当前: 1)
  - 建议值: `1`, `2`
  - 说明: 消息传递的层数，可以增强图卷积能力
  - 影响: 图神经网络的消息传递深度

### 6. **数据相关参数**
- **`window`** (当前: 10)
  - 建议值: `12`, `24`, `36`
  - 说明: 时间窗口大小，更大的窗口可以捕获更多时序信息
  - 影响: 时序建模能力

- **`adj_threshold`** (当前: 0.1)
  - 建议值: `0.05`, `0.1`, `0.2`
  - 说明: 邻接矩阵阈值，影响图的连接性
  - 影响: 图结构，影响空间信息传播

## 🟢 低优先级参数（微调）

### 7. **训练效率参数**
- **`batch_size`** (当前: 4)
  - 建议值: `8`, `16`, `32` (如果显存允许)
  - 说明: 更大的batch size可能提升训练稳定性
  - 影响: 训练稳定性和速度

- **`batches_epoch`** (当前: 300)
  - 建议值: `200`, `300`, `500`
  - 说明: 每个epoch的批次数量
  - 影响: 每个epoch的训练量

- **`precision`** (当前: 16)
  - 可选值: `16`, `32`
  - 说明: 32位精度可能更稳定，但速度慢
  - 影响: 数值精度和训练速度

## 📋 推荐的调整顺序

### 第一步：快速尝试（最可能解决问题）
```yaml
hidden_size: 32  # 从16增加到32
patience: 20     # 从5增加到20
epochs: 150      # 从80增加到150
lr: 0.001        # 从0.0008增加到0.001
```

### 第二步：如果第一步效果不明显
```yaml
hidden_size: 64  # 进一步增加容量
n_layers: 4      # 增加层数
whiten_prob: [0.1, 0.3, 0.5]  # 降低缺失值注入概率
```

### 第三步：精细调整
```yaml
window: 24       # 增加时间窗口
l2_reg: 1e-4     # 添加正则化
loss_fn: mse_loss  # 尝试L2损失
```

## ⚠️ 常见问题排查

1. **模型容量不足**: `hidden_size=16` 可能太小，优先增加
2. **训练不充分**: `epochs=80, patience=5` 可能导致过早停止
3. **学习率不合适**: 当前0.0008可能偏小，尝试0.001-0.002
4. **缺失值注入过多**: `whiten_prob=[0.2,0.5,0.8]` 可能让训练过于困难

## 💡 调试建议

1. 先单独调整`hidden_size`到32或64，观察效果
2. 增加`patience`到至少20，确保充分训练
3. **监控训练曲线，看是否有过拟合或欠拟合** - 详见 [如何判断欠拟合/过拟合](how_to_check_overfitting.md)
4. 如果验证集loss持续下降但测试集效果差，可能是过拟合，需要正则化
5. 如果训练loss不下降，可能是学习率太小或模型容量不足

## 📊 如何判断欠拟合/过拟合

**重要：** 在调整参数前，先判断模型当前是欠拟合还是过拟合，这样可以更有针对性地调整参数。

详细方法请参考：[如何判断欠拟合/过拟合](how_to_check_overfitting.md)

**快速方法：**
```bash
# 使用分析脚本（推荐）
python experiments/analyze_training_curves.py --logdir <你的日志目录>

# 或使用TensorBoard
tensorboard --logdir log/
```

