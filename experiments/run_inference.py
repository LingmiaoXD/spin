import copy
import os
import sys

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ spin æ¨¡å—
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import numpy as np
import pandas as pd
import pytorch_lightning as pl
import torch
import tsl
import yaml
from pathlib import Path
from tsl import config
from tsl.data import SpatioTemporalDataModule, ImputationDataset
from tsl.data.preprocessing import StandardScaler
from tsl.datasets import AirQuality, MetrLA, PemsBay
from tsl.imputers import Imputer
from tsl.nn.models.imputation import GRINModel
from tsl.ops.imputation import add_missing_values, sample_mask
from tsl.utils import ArgParser, parser_utils, numpy_metrics
from tsl.utils.python_utils import ensure_list

from spin.baselines import SAITS, TransformerModel, BRITS, LSTMModel
from spin.imputers import SPINImputer, SAITSImputer, BRITSImputer, LSTMImputer
from spin.models import SPINModel, SPINHierarchicalModel
from spin.datasets.lane_traffic_dataset import LaneTrafficDataset
from spin.datasets.bounded_imputation_dataset import filter_cross_boundary_windows


def get_model_classes(model_str):
    if model_str == 'spin':
        model, filler = SPINModel, SPINImputer
    elif model_str == 'spin_h':
        model, filler = SPINHierarchicalModel, SPINImputer
    elif model_str == 'grin':
        model, filler = GRINModel, Imputer
    elif model_str == 'saits':
        model, filler = SAITS, SAITSImputer
    elif model_str == 'transformer':
        model, filler = TransformerModel, SPINImputer
    elif model_str == 'brits':
        model, filler = BRITS, BRITSImputer
    elif model_str == 'lstm':
        model, filler = LSTMModel, LSTMImputer
    else:
        raise ValueError(f'Model {model_str} not available.')
    return model, filler


def get_dataset(dataset_name: str, data_path: str = None, 
                static_data_path: str = None, mask_data_path: str = None,
                feature_cols: list = None):
    """
    è·å–æ•°æ®é›†
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        data_path: åŠ¨æ€äº¤é€šæ•°æ®è·¯å¾„ (ç”¨äºlaneæ•°æ®é›†)
        static_data_path: é™æ€é“è·¯æ•°æ®è·¯å¾„ (ç”¨äºlaneæ•°æ®é›†)
        mask_data_path: æ©ç æ–‡ä»¶è·¯å¾„ (ç”¨äºlaneæ•°æ®é›†ï¼Œå¯é€‰)
        feature_cols: ç‰¹å¾åˆ—ååˆ—è¡¨ (ç”¨äºlaneæ•°æ®é›†)
    """
    # æ”¯æŒè½¦é“çº§äº¤é€šæ•°æ®é›†
    if dataset_name == 'lane':
        if static_data_path is not None and data_path is not None:
            return LaneTrafficDataset(
                static_data_path=static_data_path,
                dynamic_data_path=data_path,
                mask_data_path=mask_data_path,
                feature_cols=feature_cols,
                impute_nans=True
            )
        else:
            raise ValueError("laneæ•°æ®é›†éœ€è¦æŒ‡å®š --static-data-path å’Œ --data-path")
    
    if dataset_name.startswith('air'):
        return AirQuality(impute_nans=True, small=dataset_name[3:] == '36')
    # build missing dataset
    if dataset_name.endswith('_point'):
        p_fault, p_noise = 0., 0.25
        dataset_name = dataset_name[:-6]
    elif dataset_name.endswith('_block'):
        p_fault, p_noise = 0.0015, 0.05
        dataset_name = dataset_name[:-6]
    else:
        raise ValueError(f"Invalid dataset name: {dataset_name}.")
    if dataset_name == 'la':
        return add_missing_values(MetrLA(), p_fault=p_fault, p_noise=p_noise,
                                  min_seq=12, max_seq=12 * 4, seed=9101112)
    if dataset_name == 'bay':
        return add_missing_values(PemsBay(), p_fault=p_fault, p_noise=p_noise,
                                  min_seq=12, max_seq=12 * 4, seed=56789)
    raise ValueError(f"Invalid dataset name: {dataset_name}.")


def parse_args():
    # Argument parser
    parser = ArgParser()

    parser.add_argument("--model-name", type=str)
    parser.add_argument("--dataset-name", type=str)
    parser.add_argument("--exp-name", type=str, default=None)
    parser.add_argument("--config", type=str, default='inference.yaml')
    parser.add_argument("--root", type=str, default='log')
    
    # Lane dataset params
    parser.add_argument("--checkpoint-path", type=str, default=None,
                       help="Path to checkpoint file (.ckpt)")
    parser.add_argument("--data-path", type=str, default=None,
                       help="Path to dynamic traffic data file (csv) for lane dataset")
    parser.add_argument("--static-data-path", type=str, default=None,
                       help="Path to static road data file (graph.json) for lane dataset")
    parser.add_argument("--mask-data-path", type=str, default=None,
                       help="Path to mask data file (csv) for lane dataset")
    parser.add_argument("--feature-cols", type=str, default=None,
                       help="Comma-separated feature column names for lane dataset")

    # Data sparsity params
    parser.add_argument('--p-fault', type=float, default=0.0)
    parser.add_argument('--p-noise', type=float, default=0.75)
    parser.add_argument('--test-mask-seed', type=int, default=None)

    # Splitting/aggregation params
    parser.add_argument('--val-len', type=float, default=0.1)
    parser.add_argument('--test-len', type=float, default=0.2)
    parser.add_argument('--batch-size', type=int, default=32)

    # Connectivity params
    parser.add_argument("--adj-threshold", type=float, default=0.1)
    
    # Output params
    parser.add_argument("--output-path", type=str, default=None,
                       help="Path to save imputed results (csv file)")

    args = parser.parse_args()
    if args.config is not None:
        cfg_path = os.path.join(config.config_dir, args.config)
        if os.path.exists(cfg_path):
            with open(cfg_path, 'r') as fp:
                config_args = yaml.load(fp, Loader=yaml.FullLoader)
            for arg in config_args:
                setattr(args, arg, config_args[arg])

    return args


def load_model(exp_dir, exp_config, dm, checkpoint_path=None, u_size=None):
    model_cls, imputer_class = get_model_classes(exp_config['model_name'])
    
    # å¦‚æœæ²¡æœ‰æä¾›u_sizeï¼Œä»configä¸­è¯»å–ï¼Œæˆ–è€…ä½¿ç”¨é»˜è®¤å€¼
    if u_size is None:
        u_size = exp_config.get('u_size', 1)
    
    additional_model_hparams = dict(n_nodes=dm.n_nodes,
                                    input_size=dm.n_channels,
                                    u_size=u_size,
                                    output_size=dm.n_channels,
                                    window_size=dm.window)

    # model's inputs
    model_kwargs = parser_utils.filter_args(
        args={**exp_config, **additional_model_hparams},
        target_cls=model_cls,
        return_dict=True)

    # setup imputer
    imputer_kwargs = parser_utils.filter_argparse_args(exp_config,
                                                       imputer_class,
                                                       return_dict=True)
    
    # å¦‚æœæä¾›äº†checkpoint_pathï¼Œç›´æ¥ä»checkpointåŠ è½½
    if checkpoint_path is not None and os.path.exists(checkpoint_path):
        print(f"ä»checkpointåŠ è½½æ¨¡å‹: {checkpoint_path}")
        # ä»checkpointåŠ è½½æ—¶ï¼Œéœ€è¦æä¾›æ‰€æœ‰å¿…è¦çš„å‚æ•°
        imputer = imputer_class.load_from_checkpoint(
            checkpoint_path,
            model_class=model_cls,
            model_kwargs=model_kwargs,
            optim_class=torch.optim.Adam,
            optim_kwargs={},
            loss_fn=None,
            **imputer_kwargs
        )
        imputer.freeze()
        return imputer
    
    # å¦åˆ™ä»exp_diræŸ¥æ‰¾checkpoint
    imputer = imputer_class(
        model_class=model_cls,
        model_kwargs=model_kwargs,
        optim_class=torch.optim.Adam,
        optim_kwargs={},
        loss_fn=None,
        **imputer_kwargs
    )

    model_path = None
    if exp_dir and os.path.exists(exp_dir):
        for file in os.listdir(exp_dir):
            if file.endswith(".ckpt"):
                model_path = os.path.join(exp_dir, file)
                break
    
    if model_path is None:
        raise ValueError(f"Model not found. è¯·æä¾› --checkpoint-path æˆ–ç¡®ä¿ exp_dir ä¸­å­˜åœ¨ .ckpt æ–‡ä»¶")

    imputer.load_model(model_path)
    imputer.freeze()
    return imputer


def update_test_eval_mask(dm, dataset, p_fault, p_noise, seed=None):
    if seed is None:
        seed = np.random.randint(1e9)
    random = np.random.default_rng(seed)
    dataset.set_eval_mask(
        sample_mask(dataset.shape, p=p_fault, p_noise=p_noise,
                    min_seq=12, max_seq=36, rng=random)
    )
    dm.torch_dataset.set_mask(dataset.training_mask)
    dm.torch_dataset.update_exogenous('eval_mask', dataset.eval_mask)


def save_imputed_results_lane(y_hat, dataset, dm, output_path, 
                              train_indices=None, val_indices=None, test_indices=None,
                              mask_data_path=None):
    """
    å°†å¡«å……ç»“æœä¿å­˜ä¸ºä¸è¾“å…¥æ–‡ä»¶ç›¸åŒæ ¼å¼çš„CSVæ–‡ä»¶ï¼ˆç”¨äºlaneæ•°æ®é›†ï¼‰
    
    Args:
        y_hat: é¢„æµ‹ç»“æœï¼Œå½¢çŠ¶ä¸º [batch, window, nodes, features] æˆ– [window, nodes, features]
        dataset: LaneTrafficDataset å®ä¾‹
        dm: SpatioTemporalDataModule å®ä¾‹
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        train_indices: è®­ç»ƒé›†çš„ç´¢å¼•
        val_indices: éªŒè¯é›†çš„ç´¢å¼•
        test_indices: æµ‹è¯•é›†çš„ç´¢å¼•
        mask_data_path: æ©ç æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äºæå–å®Œæ•´çš„æ—¶é—´å’ŒèŠ‚ç‚¹åˆ—è¡¨ï¼‰
    """
    # è·å–æ•°æ®é›†çš„å…ƒä¿¡æ¯
    timestamps = dataset.timestamps
    lane_ids = dataset.lane_ids
    feature_cols = dataset.feature_cols
    time_col = dataset.time_col
    lane_id_col = dataset.lane_id_col
    mask_time_col = dataset.mask_time_col
    mask_lane_col = dataset.mask_lane_col
    
    # å¦‚æœæä¾›äº†maskæ–‡ä»¶è·¯å¾„ï¼Œå°è¯•ä»maskæ–‡ä»¶ä¸­æå–å®Œæ•´çš„æ—¶é—´å’ŒèŠ‚ç‚¹åˆ—è¡¨
    if mask_data_path is not None and os.path.exists(mask_data_path):
        print(f"ä»maskæ–‡ä»¶ä¸­æå–å®Œæ•´çš„æ—¶é—´å’ŒèŠ‚ç‚¹åˆ—è¡¨: {mask_data_path}")
        try:
            mask_df = pd.read_csv(mask_data_path)
            if mask_time_col in mask_df.columns and mask_lane_col in mask_df.columns:
                # ä»maskæ–‡ä»¶ä¸­æå–æ‰€æœ‰å”¯ä¸€çš„æ—¶é—´å’ŒèŠ‚ç‚¹
                mask_timestamps = np.sort(mask_df[mask_time_col].unique())
                mask_lane_ids = np.sort(mask_df[mask_lane_col].unique())
                
                # åˆå¹¶datasetå’Œmaskä¸­çš„æ—¶é—´å’ŒèŠ‚ç‚¹ï¼ˆå–å¹¶é›†ï¼‰
                all_timestamps = np.sort(np.unique(np.concatenate([timestamps, mask_timestamps])))
                all_lane_ids = np.sort(np.unique(np.concatenate([lane_ids, mask_lane_ids])))
                
                print(f"   datasetä¸­çš„æ—¶é—´æ•°: {len(timestamps)}, èŠ‚ç‚¹æ•°: {len(lane_ids)}")
                print(f"   maskæ–‡ä»¶ä¸­çš„æ—¶é—´æ•°: {len(mask_timestamps)}, èŠ‚ç‚¹æ•°: {len(mask_lane_ids)}")
                print(f"   åˆå¹¶åçš„æ—¶é—´æ•°: {len(all_timestamps)}, èŠ‚ç‚¹æ•°: {len(all_lane_ids)}")
                
                # ä½¿ç”¨å®Œæ•´åˆ—è¡¨
                timestamps = all_timestamps
                lane_ids = all_lane_ids
            else:
                print(f"âš ï¸ è­¦å‘Š: maskæ–‡ä»¶ç¼ºå°‘å¿…éœ€åˆ— {mask_time_col} æˆ– {mask_lane_col}ï¼Œä½¿ç”¨datasetä¸­çš„æ—¶é—´å’ŒèŠ‚ç‚¹åˆ—è¡¨")
        except Exception as e:
            print(f"âš ï¸ è­¦å‘Š: æ— æ³•ä»maskæ–‡ä»¶æå–å®Œæ•´åˆ—è¡¨: {e}ï¼Œä½¿ç”¨datasetä¸­çš„æ—¶é—´å’ŒèŠ‚ç‚¹åˆ—è¡¨")
    elif hasattr(dataset, 'mask_data_paths') and dataset.mask_data_paths:
        # å°è¯•ä»datasetçš„mask_data_pathsä¸­åŠ è½½
        for mask_path in dataset.mask_data_paths:
            if mask_path is not None and os.path.exists(mask_path):
                print(f"ä»datasetçš„maskæ–‡ä»¶ä¸­æå–å®Œæ•´çš„æ—¶é—´å’ŒèŠ‚ç‚¹åˆ—è¡¨: {mask_path}")
                try:
                    mask_df = pd.read_csv(mask_path)
                    if mask_time_col in mask_df.columns and mask_lane_col in mask_df.columns:
                        mask_timestamps = np.sort(mask_df[mask_time_col].unique())
                        mask_lane_ids = np.sort(mask_df[mask_lane_col].unique())
                        
                        all_timestamps = np.sort(np.unique(np.concatenate([timestamps, mask_timestamps])))
                        all_lane_ids = np.sort(np.unique(np.concatenate([lane_ids, mask_lane_ids])))
                        
                        print(f"   datasetä¸­çš„æ—¶é—´æ•°: {len(timestamps)}, èŠ‚ç‚¹æ•°: {len(lane_ids)}")
                        print(f"   maskæ–‡ä»¶ä¸­çš„æ—¶é—´æ•°: {len(mask_timestamps)}, èŠ‚ç‚¹æ•°: {len(mask_lane_ids)}")
                        print(f"   åˆå¹¶åçš„æ—¶é—´æ•°: {len(all_timestamps)}, èŠ‚ç‚¹æ•°: {len(all_lane_ids)}")
                        
                        timestamps = all_timestamps
                        lane_ids = all_lane_ids
                        break
                except Exception as e:
                    print(f"âš ï¸ è­¦å‘Š: æ— æ³•ä»maskæ–‡ä»¶ {mask_path} æå–å®Œæ•´åˆ—è¡¨: {e}")
                    continue
    
    # å¤„ç†y_hatçš„å½¢çŠ¶
    original_shape = y_hat.shape
    print(f"åŸå§‹y_hatå½¢çŠ¶: {original_shape}")
    
    # å¦‚æœæ˜¯4ç»´ [batch, window, nodes, features]ï¼Œéœ€è¦å±•å¹³
    if len(y_hat.shape) == 4:
        batch_size, window_size, n_nodes, n_features = y_hat.shape
        # å±•å¹³æ‰¹æ¬¡å’Œçª—å£ç»´åº¦
        y_hat = y_hat.reshape(-1, n_nodes, n_features)
        print(f"å±•å¹³åå½¢çŠ¶: {y_hat.shape}")
    
    # ç¡®ä¿æ˜¯3ç»´ [time, nodes, features]
    if len(y_hat.shape) != 3:
        raise ValueError(f"y_hatåº”è¯¥æ˜¯3ç»´æˆ–4ç»´ï¼Œä½†å¾—åˆ°: {original_shape}")
    
    n_time_steps, n_nodes, n_features = y_hat.shape
    
    # è·å–çª—å£å’Œæ­¥é•¿ä¿¡æ¯ï¼ˆæå‰è·å–ï¼Œç”¨äºè°ƒè¯•ä¿¡æ¯ï¼‰
    window = dm.torch_dataset.window
    stride = dm.torch_dataset.stride
    print(f"y_hatå½¢çŠ¶: {y_hat.shape}, window={window}, stride={stride}")
    
    # åæ ‡å‡†åŒ–
    scaler = dm.scalers.get('data')
    if scaler is not None:
        print("æ‰§è¡Œåæ ‡å‡†åŒ–...")
        # scaleræœŸæœ› Tensor è¾“å…¥ï¼Œéœ€è¦å…ˆè½¬æ¢
        y_hat_tensor = torch.from_numpy(y_hat).float()
        y_hat_inv = scaler.inverse_transform(y_hat_tensor)
        # è½¬æ¢å› numpy
        if isinstance(y_hat_inv, torch.Tensor):
            y_hat = y_hat_inv.cpu().numpy()
        else:
            y_hat = y_hat_inv
    
    # åå½’ä¸€åŒ– avg_speedï¼ˆå¦‚æœæ•°æ®é›†ä¿å­˜äº†å½’ä¸€åŒ–å‚æ•°ï¼‰
    # æ³¨æ„ï¼šStandardScaler åæ ‡å‡†åŒ–åå¾—åˆ°çš„æ˜¯å½’ä¸€åŒ–åçš„å€¼ï¼ˆ0-1ï¼‰ï¼Œ
    # å¦‚æœåŸå§‹ avg_speed æ˜¯ç»å¯¹é€Ÿåº¦å€¼ï¼Œéœ€è¦åå½’ä¸€åŒ–å›åŸå§‹èŒƒå›´
    if hasattr(dataset, 'speed_normalization_params') and dataset.speed_normalization_params is not None:
        norm_params = dataset.speed_normalization_params
        if not norm_params.get('is_normalized', True):  # å¦‚æœåŸå§‹å€¼ä¸æ˜¯å½’ä¸€åŒ–çš„
            speed_idx = norm_params.get('feature_idx')
            if speed_idx is not None and speed_idx < y_hat.shape[-1]:
                speed_min = norm_params['speed_min']
                speed_max = norm_params['speed_max']
                speed_range = speed_max - speed_min
                if speed_range > 1e-6:
                    # åå½’ä¸€åŒ–ï¼šä» [0, 1] æ¢å¤åˆ° [speed_min, speed_max]
                    y_hat[..., speed_idx] = y_hat[..., speed_idx] * speed_range + speed_min
                    print(f"âœ… å·²å°† avg_speed ä»å½’ä¸€åŒ–å€¼ [0, 1] åå½’ä¸€åŒ–å›ç»å¯¹é€Ÿåº¦å€¼ [{speed_min:.2f}, {speed_max:.2f}] km/h")
                else:
                    print(f"âš ï¸ é€Ÿåº¦èŒƒå›´è¿‡å°ï¼Œè·³è¿‡åå½’ä¸€åŒ–")
    
    # è·å–æµ‹è¯•é›†çš„ç´¢å¼•
    if test_indices is None:
        # å¦‚æœtest_indicesä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤çš„splitterå‚æ•°
        splitter = dataset.get_splitter(val_len=0.1, test_len=0.2)
        train_idx, val_idx, test_idx = splitter.split(dataset)
        test_indices = test_idx
    
    # ç”±äºä½¿ç”¨äº†çª—å£åŒ–ï¼Œy_hatå¯èƒ½åŒ…å«é‡å çš„çª—å£
    # æˆ‘ä»¬éœ€è¦ä»torch_datasetè·å–å®é™…çš„ç´¢å¼•æ˜ å°„
    # ç®€åŒ–å¤„ç†ï¼šå‡è®¾y_hatå¯¹åº”æµ‹è¯•é›†çš„æ‰€æœ‰çª—å£
    # å¯¹äºçª—å£åŒ–çš„æ•°æ®ï¼Œæˆ‘ä»¬å–æ¯ä¸ªçª—å£çš„æœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼ˆæˆ–ä¸­é—´æ—¶é—´æ­¥ï¼‰
    window = dm.torch_dataset.window
    
    # æ„å»ºå®Œæ•´çš„æ—¶é—´åºåˆ—æ•°æ®
    # ç”±äºtimestampså’Œlane_idså¯èƒ½å·²ç»æ‰©å±•ï¼ˆä»maskæ–‡ä»¶ï¼‰ï¼Œéœ€è¦æ‰©å±•æ•°æ®çŸ©é˜µ
    original_timestamps = dataset.timestamps
    original_lane_ids = dataset.lane_ids
    
    # åˆ›å»ºåŸå§‹ç´¢å¼•æ˜ å°„
    original_time_to_idx = {t: idx for idx, t in enumerate(original_timestamps)}
    original_lane_to_idx = {lid: idx for idx, lid in enumerate(original_lane_ids)}
    
    # åˆ›å»ºæ–°ç´¢å¼•æ˜ å°„
    new_time_to_idx = {t: idx for idx, t in enumerate(timestamps)}
    new_lane_to_idx = {lid: idx for idx, lid in enumerate(lane_ids)}
    
    # å¦‚æœæ—¶é—´å’ŒèŠ‚ç‚¹åˆ—è¡¨å·²æ‰©å±•ï¼Œéœ€è¦æ‰©å±•æ•°æ®çŸ©é˜µ
    if len(timestamps) > len(original_timestamps) or len(lane_ids) > len(original_lane_ids):
        print(f"æ‰©å±•æ•°æ®çŸ©é˜µ: ä» {dataset.data.shape} åˆ° ({len(timestamps)}, {len(lane_ids)}, {len(feature_cols)})")
        # åˆ›å»ºæ–°çš„æ•°æ®çŸ©é˜µ
        n_times = len(timestamps)
        n_lanes = len(lane_ids)
        n_features = len(feature_cols)
        
        full_data = np.full((n_times, n_lanes, n_features), np.nan)
        training_mask = np.zeros((n_times, n_lanes, n_features), dtype=bool)
        
        # ä»åŸå§‹æ•°æ®çŸ©é˜µå¤åˆ¶æ•°æ®
        for orig_t_idx, orig_t in enumerate(original_timestamps):
            new_t_idx = new_time_to_idx.get(orig_t)
            if new_t_idx is not None:
                for orig_l_idx, orig_l in enumerate(original_lane_ids):
                    new_l_idx = new_lane_to_idx.get(orig_l)
                    if new_l_idx is not None:
                        full_data[new_t_idx, new_l_idx, :] = dataset.data[orig_t_idx, orig_l_idx, :]
                        training_mask[new_t_idx, new_l_idx, :] = dataset.training_mask[orig_t_idx, orig_l_idx, :]
    else:
        # ä½¿ç”¨åŸå§‹æ•°æ®çŸ©é˜µ
        full_data = dataset.data.copy()  # [time, nodes, features]
        training_mask = dataset.training_mask.copy()  # [time, nodes, features]
    
    # å¯¹ full_data ä¸­çš„ avg_speed ä¹Ÿè¿›è¡Œåå½’ä¸€åŒ–ï¼ˆå¦‚æœè¢«å½’ä¸€åŒ–äº†ï¼‰
    # æ³¨æ„ï¼šdataset.data ä¸­çš„ avg_speed åœ¨é¢„å¤„ç†æ—¶å¯èƒ½å·²è¢«å½’ä¸€åŒ–åˆ° [0, 1]
    # éœ€è¦å°†å…¶åå½’ä¸€åŒ–å›åŸå§‹ç»å¯¹é€Ÿåº¦å€¼ï¼Œä»¥ä¿æŒè¾“å‡ºç»“æœçš„ä¸€è‡´æ€§
    # è¿™æ ·å·²çŸ¥å€¼ä½ç½®çš„å€¼å°±æ˜¯åŸå§‹å€¼ï¼Œè€Œä¸æ˜¯å½’ä¸€åŒ–åçš„å€¼
    if hasattr(dataset, 'speed_normalization_params') and dataset.speed_normalization_params is not None:
        norm_params = dataset.speed_normalization_params
        if not norm_params.get('is_normalized', True):  # å¦‚æœåŸå§‹å€¼ä¸æ˜¯å½’ä¸€åŒ–çš„
            speed_idx = norm_params.get('feature_idx')
            if speed_idx is not None and speed_idx < full_data.shape[-1]:
                speed_min = norm_params['speed_min']
                speed_max = norm_params['speed_max']
                speed_range = speed_max - speed_min
                if speed_range > 1e-6:
                    # åå½’ä¸€åŒ–ï¼šä» [0, 1] æ¢å¤åˆ° [speed_min, speed_max]
                    # ç›´æ¥å¯¹æ•´ä¸ª speed_idx ç‰¹å¾è¿›è¡Œåå½’ä¸€åŒ–
                    full_data[..., speed_idx] = full_data[..., speed_idx] * speed_range + speed_min
                    print(f"âœ… å·²å°† full_data ä¸­çš„ avg_speed ä»å½’ä¸€åŒ–å€¼ [0, 1] åå½’ä¸€åŒ–å›ç»å¯¹é€Ÿåº¦å€¼ [{speed_min:.2f}, {speed_max:.2f}] km/h")
    
    # å°†çª—å£åŒ–çš„é¢„æµ‹ç»“æœæ˜ å°„å›å®Œæ•´æ—¶é—´åºåˆ—
    # æ³¨æ„ï¼šwindowå’Œstrideå·²ç»åœ¨å‰é¢è·å–äº†
    # y_hat çš„å½¢çŠ¶æ˜¯ [num_windows, window, nodes, features] æˆ– [num_windows * window, nodes, features]
    # éœ€è¦æ ¹æ®çª—å£çš„èµ·å§‹ä½ç½®æ˜ å°„å›åŸå§‹æ—¶é—´åºåˆ—
    
    # è·å–æ‰€æœ‰æ•°æ®é›†çš„ç´¢å¼•ï¼ˆç”¨äºç¡®å®šçª—å£çš„èµ·å§‹ä½ç½®ï¼‰
    all_indices = []
    if train_indices is not None:
        all_indices.extend(train_indices)
    if val_indices is not None:
        all_indices.extend(val_indices)
    if test_indices is not None:
        all_indices.extend(test_indices)
    
    # è½¬æ¢ç´¢å¼•ä¸ºæ•´æ•°ï¼ˆå¦‚æœæ˜¯æ—¶é—´æˆ³ï¼‰
    def convert_to_int_indices(indices):
        if indices is None or len(indices) == 0:
            return []
        first_idx = indices[0]
        if isinstance(first_idx, (str, pd.Timestamp)) or hasattr(first_idx, 'strftime'):
            time_to_idx = {t: idx for idx, t in enumerate(timestamps)}
            int_indices = [time_to_idx.get(t, -1) for t in indices]
            return [idx for idx in int_indices if idx >= 0]
        else:
            return [int(idx) for idx in indices]
    
    train_int = convert_to_int_indices(train_indices)
    val_int = convert_to_int_indices(val_indices)
    test_int = convert_to_int_indices(test_indices)
    all_int = sorted(set(train_int + val_int + test_int))
    
    # è®¡ç®—æ¯ä¸ªæ•°æ®é›†å¯¹åº”çš„çª—å£èŒƒå›´
    # ç”±äºçª—å£åŒ–ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“æ¯ä¸ªçª—å£å¯¹åº”çš„æ—¶é—´æ­¥
    # ç®€åŒ–å¤„ç†ï¼šå‡è®¾çª—å£æŒ‰é¡ºåºæ’åˆ—ï¼Œæ¯ä¸ªçª—å£çš„æœ€åä¸€ä¸ªæ—¶é—´æ­¥å¯¹åº”é¢„æµ‹å€¼
    
    # ç†è§£y_hatçš„ç»“æ„ï¼š
    # y_hatçš„å½¢çŠ¶åº”è¯¥æ˜¯ [num_windows * window, nodes, features] æˆ– [num_windows, window, nodes, features]
    # å…¶ä¸­num_windowsæ˜¯æ ¹æ®åŸå§‹æ—¶é—´åºåˆ—é•¿åº¦ã€windowå’Œstrideè®¡ç®—å¾—å‡ºçš„
    
    # è®¡ç®—å®é™…åº”è¯¥æœ‰çš„çª—å£æ•°é‡ï¼ˆåŸºäºåŸå§‹æ—¶é—´åºåˆ—ï¼‰
    original_n_times = len(original_timestamps)
    if original_n_times >= window:
        # çª—å£æ•°é‡ = (æ€»æ—¶é—´æ­¥æ•° - çª—å£å¤§å°) / æ­¥é•¿ + 1
        expected_num_windows = (original_n_times - window) // stride + 1
    else:
        expected_num_windows = 1 if original_n_times > 0 else 0
    
    print(f"çª—å£æ˜ å°„ä¿¡æ¯:")
    print(f"   åŸå§‹æ—¶é—´æ­¥æ•°: {original_n_times}, window={window}, stride={stride}")
    print(f"   é¢„æœŸçª—å£æ•°é‡: {expected_num_windows}")
    print(f"   y_hatå½¢çŠ¶: {y_hat.shape}, n_time_steps={n_time_steps}")
    
    # å°†y_haté‡å¡‘ä¸º [num_windows, window, nodes, features]
    if len(y_hat.shape) == 3:
        # å¦‚æœæ˜¯ [time, nodes, features]ï¼Œéœ€è¦é‡å¡‘
        # n_time_stepsåº”è¯¥æ˜¯ num_windows * window
        if n_time_steps == expected_num_windows * window:
            num_windows = expected_num_windows
            y_hat_reshaped = y_hat.reshape(num_windows, window, n_nodes, n_features)
        elif n_time_steps % window == 0:
            # å¦‚æœèƒ½è¢«windowæ•´é™¤ï¼Œä½¿ç”¨å®é™…çš„æ—¶é—´æ­¥æ•°
            num_windows = n_time_steps // window
            y_hat_reshaped = y_hat.reshape(num_windows, window, n_nodes, n_features)
            print(f"   âš ï¸ æ³¨æ„: y_hatçš„æ—¶é—´æ­¥æ•°({n_time_steps})ä¸é¢„æœŸ({expected_num_windows * window})ä¸åŒ¹é…")
            print(f"   ä½¿ç”¨å®é™…çª—å£æ•°é‡: {num_windows}")
        else:
            # å¦‚æœä¸èƒ½æ•´é™¤ï¼Œå¯èƒ½éœ€è¦å¡«å……æˆ–æˆªæ–­
            num_windows = (n_time_steps + window - 1) // window  # å‘ä¸Šå–æ•´
            target_size = num_windows * window
            if n_time_steps < target_size:
                # å¡«å……
                padding = np.zeros((target_size - n_time_steps, n_nodes, n_features))
                y_hat_padded = np.concatenate([y_hat, padding], axis=0)
                y_hat_reshaped = y_hat_padded.reshape(num_windows, window, n_nodes, n_features)
            else:
                # æˆªæ–­
                y_hat_reshaped = y_hat[:target_size].reshape(num_windows, window, n_nodes, n_features)
    else:
        # å¦‚æœå·²ç»æ˜¯4ç»´ï¼Œç›´æ¥ä½¿ç”¨
        y_hat_reshaped = y_hat
        num_windows = y_hat.shape[0]
    
    # æ˜ å°„çª—å£é¢„æµ‹åˆ°å®Œæ•´æ—¶é—´åºåˆ—
    # æ³¨æ„ï¼šy_hatåªåŒ…å«åŸå§‹æ—¶é—´èŒƒå›´å†…çš„é¢„æµ‹ç»“æœï¼Œéœ€è¦æ˜ å°„åˆ°åŸå§‹æ—¶é—´èŒƒå›´
    # å¯¹äºæ–°å¢çš„æ—¶é—´ï¼ˆåœ¨maskæ–‡ä»¶ä¸­ä½†ä¸åœ¨åŸå§‹æ•°æ®ä¸­ï¼‰ï¼Œè¿™äº›ä½ç½®ä¿æŒä¸ºNaN
    original_n_times = len(original_timestamps)
    
    # åˆ›å»ºä¸€ä¸ªæ•°ç»„æ¥è·Ÿè¸ªæ¯ä¸ªæ—¶é—´æ­¥æ˜¯å¦å·²ç»è¢«é¢„æµ‹å€¼è¦†ç›–
    # å¯¹äºé‡å çš„çª—å£ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªçª—å£çš„å€¼
    time_covered = np.zeros(original_n_times, dtype=bool)
    
    # æ˜ å°„çª—å£é¢„æµ‹åˆ°åŸå§‹æ—¶é—´åºåˆ—
    # æ ¹æ®ImputationDatasetçš„çª—å£åˆ›å»ºé€»è¾‘ï¼Œçª—å£æ˜¯æŒ‰strideæ­¥é•¿åˆ›å»ºçš„
    window_idx = 0
    for start_time in range(0, original_n_times - window + 1, stride):
        if window_idx >= num_windows:
            break
        
        # è·å–è¯¥çª—å£çš„æ‰€æœ‰æ—¶é—´æ­¥é¢„æµ‹ [window, nodes, features]
        window_preds = y_hat_reshaped[window_idx, :, :, :]
        # å°†è¯¥çª—å£çš„æ‰€æœ‰æ—¶é—´æ­¥æ˜ å°„å›åŸå§‹æ—¶é—´åºåˆ—
        for w in range(window):
            orig_time_idx = start_time + w
            if orig_time_idx < original_n_times:
                # è·å–åŸå§‹æ—¶é—´æˆ³
                orig_timestamp = original_timestamps[orig_time_idx]
                # æ‰¾åˆ°åœ¨æ–°æ—¶é—´åˆ—è¡¨ä¸­çš„ç´¢å¼•
                new_time_idx = new_time_to_idx.get(orig_timestamp)
                
                if new_time_idx is not None:
                    # window_predçš„å½¢çŠ¶æ˜¯[åŸå§‹èŠ‚ç‚¹æ•°, features]ï¼Œéœ€è¦åªæ˜ å°„åˆ°åŸå§‹èŠ‚ç‚¹èŒƒå›´
                    window_pred = window_preds[w, :, :]  # [åŸå§‹èŠ‚ç‚¹æ•°, features]
                    
                    # åªæ˜ å°„åˆ°åŸå§‹èŠ‚ç‚¹èŒƒå›´å†…çš„æ•°æ®
                    for orig_l_idx, orig_lane_id in enumerate(original_lane_ids):
                        new_l_idx = new_lane_to_idx.get(orig_lane_id)
                        if new_l_idx is not None:
                            # å¯¹äºç¼ºå¤±å€¼ä½ç½®ï¼Œä½¿ç”¨é¢„æµ‹å€¼ï¼›å¯¹äºå·²çŸ¥å€¼ä½ç½®ï¼Œä¿ç•™åŸå§‹å€¼
                            mask_missing = ~training_mask[new_time_idx, new_l_idx, :]  # ç¼ºå¤±å€¼çš„ä½ç½® [features]
                            window_pred_lane = window_pred[orig_l_idx, :]  # [features]
                            full_data[new_time_idx, new_l_idx, :] = np.where(
                                mask_missing, window_pred_lane, full_data[new_time_idx, new_l_idx, :]
                            )
                    
                    time_covered[orig_time_idx] = True
        
        window_idx += 1
    
    # å¤„ç†æœ€åä¸€ä¸ªçª—å£ï¼ˆå¦‚æœæ—¶é—´åºåˆ—é•¿åº¦ä¸èƒ½è¢«strideæ•´é™¤ï¼Œæœ€åä¸€ä¸ªçª—å£å¯èƒ½æ²¡æœ‰å®Œå…¨è¦†ç›–ï¼‰
    if original_n_times > 0:
        # è®¡ç®—æœ€åä¸€ä¸ªçª—å£çš„èµ·å§‹ä½ç½®ï¼ˆç¡®ä¿è¦†ç›–æœ€åå‡ ä¸ªæ—¶é—´ç‚¹ï¼‰
        last_start = max(0, original_n_times - window)
        if last_start >= 0 and window_idx < num_windows:
            window_preds = y_hat_reshaped[window_idx, :, :, :]
            for w in range(window):
                orig_time_idx = last_start + w
                if orig_time_idx < original_n_times:
                    # å¦‚æœè¿™ä¸ªæ—¶é—´ç‚¹è¿˜æ²¡æœ‰è¢«è¦†ç›–ï¼Œæˆ–è€…éœ€è¦æ›´æ–°ï¼ˆä½¿ç”¨æœ€åä¸€ä¸ªçª—å£çš„å€¼ï¼‰
                    orig_timestamp = original_timestamps[orig_time_idx]
                    new_time_idx = new_time_to_idx.get(orig_timestamp)
                    
                    if new_time_idx is not None:
                        # window_predçš„å½¢çŠ¶æ˜¯[åŸå§‹èŠ‚ç‚¹æ•°, features]ï¼Œéœ€è¦åªæ˜ å°„åˆ°åŸå§‹èŠ‚ç‚¹èŒƒå›´
                        window_pred = window_preds[w, :, :]  # [åŸå§‹èŠ‚ç‚¹æ•°, features]
                        
                        # åªæ˜ å°„åˆ°åŸå§‹èŠ‚ç‚¹èŒƒå›´å†…çš„æ•°æ®
                        for orig_l_idx, orig_lane_id in enumerate(original_lane_ids):
                            new_l_idx = new_lane_to_idx.get(orig_lane_id)
                            if new_l_idx is not None:
                                # å¯¹äºç¼ºå¤±å€¼ä½ç½®ï¼Œä½¿ç”¨é¢„æµ‹å€¼ï¼›å¯¹äºå·²çŸ¥å€¼ä½ç½®ï¼Œä¿ç•™åŸå§‹å€¼
                                mask_missing = ~training_mask[new_time_idx, new_l_idx, :]  # ç¼ºå¤±å€¼çš„ä½ç½® [features]
                                window_pred_lane = window_pred[orig_l_idx, :]  # [features]
                                full_data[new_time_idx, new_l_idx, :] = np.where(
                                    mask_missing, window_pred_lane, full_data[new_time_idx, new_l_idx, :]
                                )
                        
                        time_covered[orig_time_idx] = True
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´ç‚¹æ²¡æœ‰è¢«è¦†ç›–
    uncovered_times = np.where(~time_covered)[0]
    if len(uncovered_times) > 0:
        print(f"âš ï¸ æ³¨æ„: æœ‰ {len(uncovered_times)} ä¸ªæ—¶é—´ç‚¹æ²¡æœ‰è¢«çª—å£è¦†ç›–ï¼ˆè¿™æ˜¯æ­£å¸¸çš„ï¼Œå¦‚æœstride>1æˆ–çª—å£æœªå®Œå…¨è¦†ç›–ï¼‰")
        print(f"   æœªè¦†ç›–çš„æ—¶é—´ç‚¹ç´¢å¼•: {uncovered_times[:20] if len(uncovered_times) <= 20 else str(uncovered_times[:20]) + '...'}")
        print(f"   åŸå§‹æ—¶é—´èŒƒå›´: 0 åˆ° {original_n_times-1}, window={window}, stride={stride}")
        print(f"   çª—å£æ•°é‡: {num_windows}, y_hatå½¢çŠ¶: {y_hat.shape}")
        print(f"   è¿™äº›æ—¶é—´ç‚¹å°†ä¿æŒä¸ºåŸå§‹å€¼æˆ–NaNï¼ˆä¸ä¼šè¢«é¢„æµ‹å€¼è¦†ç›–ï¼‰")
    
    # ç»Ÿè®¡è¦†ç›–æƒ…å†µ
    coverage_ratio = time_covered.sum() / original_n_times if original_n_times > 0 else 0
    print(f"âœ… æ—¶é—´ç‚¹è¦†ç›–æƒ…å†µ: {time_covered.sum()}/{original_n_times} ({coverage_ratio:.1%})")
    
    # æ„å»ºDataFrame
    # æ³¨æ„ï¼šå…ˆéå†lane_idï¼Œå†éå†timestampï¼Œè¿™æ ·æ„å»ºçš„æ•°æ®å·²ç»æ˜¯æŒ‰lane_idä¼˜å…ˆçš„é¡ºåº
    result_rows = []
    for n_idx, lane_id in enumerate(lane_ids):
        for t_idx, timestamp in enumerate(timestamps):
            row = {
                lane_id_col: lane_id,  # lane_idæ”¾åœ¨ç¬¬ä¸€åˆ—
                time_col: timestamp     # time_colæ”¾åœ¨ç¬¬äºŒåˆ—
            }
            # æ·»åŠ æ‰€æœ‰ç‰¹å¾åˆ—
            for f_idx, feature_name in enumerate(feature_cols):
                if f_idx < full_data.shape[-1]:
                    row[feature_name] = full_data[t_idx, n_idx, f_idx]
                else:
                    row[feature_name] = np.nan
            result_rows.append(row)
    
    # åˆ›å»ºDataFrameå¹¶ä¿å­˜
    result_df = pd.DataFrame(result_rows)
    
    # æŒ‰lane_idä¼˜å…ˆæ’åºï¼Œç„¶åæŒ‰æ—¶é—´æ’åºï¼ˆç¡®ä¿é¡ºåºï¼šlane_id=0çš„æ‰€æœ‰æ—¶é—´ï¼Œlane_id=1çš„æ‰€æœ‰æ—¶é—´...ï¼‰
    result_df = result_df.sort_values([lane_id_col, time_col])
    
    # è°ƒæ•´åˆ—é¡ºåºï¼šç¡®ä¿lane_id_colæ˜¯ç¬¬ä¸€åˆ—ï¼Œtime_colæ˜¯ç¬¬äºŒåˆ—ï¼Œç„¶åæ˜¯ç‰¹å¾åˆ—
    column_order = [lane_id_col, time_col] + feature_cols
    # åªä¿ç•™å®é™…å­˜åœ¨çš„åˆ—
    column_order = [col for col in column_order if col in result_df.columns]
    result_df = result_df[column_order]
    
    # è¿‡æ»¤æ‰æ‰€æœ‰ç‰¹å¾åˆ—éƒ½æ˜¯NaNçš„è¡Œ
    # æ³¨æ„ï¼šåªæ£€æŸ¥ç‰¹å¾åˆ—ï¼ˆfeature_colsï¼‰ï¼Œä¸åŒ…æ‹¬lane_id_colå’Œtime_col
    # å¦‚æœåªæœ‰lane_id_colå’Œtime_colæœ‰å€¼ï¼Œä½†æ‰€æœ‰ç‰¹å¾åˆ—éƒ½æ˜¯NaNï¼Œåˆ™ä¸ç®—æœ‰æ•ˆæ•°æ®ï¼Œä¼šè¢«è¿‡æ»¤æ‰
    feature_cols_in_df = [col for col in feature_cols if col in result_df.columns]
    if len(feature_cols_in_df) > 0:
        # æ£€æŸ¥æ¯ä¸€è¡Œï¼Œå¦‚æœæ‰€æœ‰ç‰¹å¾åˆ—éƒ½æ˜¯NaNï¼Œåˆ™è¿‡æ»¤æ‰
        # ä½¿ç”¨any()æ£€æŸ¥æ˜¯å¦æœ‰è‡³å°‘ä¸€ä¸ªç‰¹å¾åˆ—ä¸æ˜¯NaN
        # åªæ£€æŸ¥ç‰¹å¾åˆ—ï¼Œä¸æ£€æŸ¥lane_id_colå’Œtime_col
        has_valid_data = result_df[feature_cols_in_df].notna().any(axis=1)
        rows_before = len(result_df)
        result_df = result_df[has_valid_data].copy()
        rows_after = len(result_df)
        rows_filtered = rows_before - rows_after
        
        if rows_filtered > 0:
            print(f"ğŸ“ è¿‡æ»¤æ‰ {rows_filtered} è¡Œï¼ˆæ‰€æœ‰ç‰¹å¾åˆ—éƒ½æ˜¯NaNï¼Œå³ä½¿lane_idå’Œtime_colæœ‰å€¼ä¹Ÿä¸ç®—æœ‰æ•ˆæ•°æ®ï¼‰")
    else:
        print(f"âš ï¸ è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°ç‰¹å¾åˆ—ï¼Œæ— æ³•è¿‡æ»¤æ— æ•ˆæ•°æ®")
    
    # æ ¼å¼åŒ–æ•°å€¼åˆ—ï¼šæ•´æ•°ä¿æŒæ•´æ•°ï¼Œ1ä½å°æ•°ä¿æŒ1ä½ï¼Œ2ä½åŠä»¥ä¸Šå››èˆäº”å…¥åˆ°2ä½
    def format_number(x):
        """æ ¼å¼åŒ–æ•°å€¼ï¼šä¿æŒæ•´æ•°å’Œ1ä½å°æ•°çš„åŸå§‹æ ¼å¼ï¼Œ2ä½åŠä»¥ä¸Šå››èˆäº”å…¥åˆ°2ä½"""
        if pd.isna(x):
            return x
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ•´æ•°ï¼ˆè€ƒè™‘æµ®ç‚¹è¯¯å·®ï¼‰
        if abs(x - round(x)) < 1e-10:
            return int(round(x))
        # æ£€æŸ¥æ˜¯å¦ä¸º1ä½å°æ•°
        rounded_1 = round(x, 1)
        if abs(x - rounded_1) < 1e-10:
            return rounded_1
        # å¦åˆ™å››èˆäº”å…¥åˆ°2ä½å°æ•°
        return round(x, 2)
    
    numeric_cols = [col for col in result_df.columns 
                   if col not in [time_col, lane_id_col]]
    for col in numeric_cols:
        result_df[col] = result_df[col].apply(format_number)
    
    # ä¿å­˜ä¸ºCSV
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    result_df.to_csv(output_path, index=False)
    print(f"âœ… å¡«å……ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    print(f"   å…± {len(result_df)} æ¡è®°å½•ï¼Œ{len(feature_cols)} ä¸ªç‰¹å¾åˆ—")
    if len(result_df) > 0:
        print(f"   æ—¶é—´èŒƒå›´: {result_df[time_col].min()} åˆ° {result_df[time_col].max()}")
    
    return result_df


def run_experiment(args):
    # Set configuration
    args = copy.deepcopy(args)
    tsl.logger.disabled = True

    # script flags
    is_spin = args.model_name in ['spin', 'spin_h']
    is_lstm = args.model_name == 'lstm'

    ########################################
    # load config                          #
    ########################################

    exp_dir = None
    exp_config = {}
    
    # å¦‚æœæä¾›äº†checkpoint_pathï¼Œå°è¯•ä»checkpointæ‰€åœ¨ç›®å½•åŠ è½½config
    if args.checkpoint_path is not None and os.path.exists(args.checkpoint_path):
        checkpoint_dir = os.path.dirname(args.checkpoint_path)
        config_path = os.path.join(checkpoint_dir, 'config.yaml')
        if os.path.exists(config_path):
            with open(config_path, 'r') as fp:
                exp_config = yaml.load(fp, Loader=yaml.FullLoader)
            exp_dir = checkpoint_dir
            print(f"ä»checkpointç›®å½•åŠ è½½é…ç½®: {config_path}")
        else:
            print(f"è­¦å‘Š: checkpointç›®å½•ä¸­æœªæ‰¾åˆ°config.yamlï¼Œå°†ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°")
    
    # å¦‚æœæ²¡æœ‰ä»checkpointåŠ è½½åˆ°configï¼Œå°è¯•ä»exp_diråŠ è½½
    if not exp_config and args.exp_name is not None:
        if args.root is None:
            root = tsl.config.log_dir
        else:
            root = os.path.join(tsl.config.curr_dir, args.root)
        exp_dir = os.path.join(root, args.dataset_name,
                               args.model_name, args.exp_name)
        config_path = os.path.join(exp_dir, 'config.yaml')
        if os.path.exists(config_path):
            with open(config_path, 'r') as fp:
                exp_config = yaml.load(fp, Loader=yaml.FullLoader)
            print(f"ä»å®éªŒç›®å½•åŠ è½½é…ç½®: {config_path}")
    
    # å¦‚æœä»ç„¶æ²¡æœ‰configï¼Œä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ä½œä¸ºé»˜è®¤å€¼
    if not exp_config:
        print("è­¦å‘Š: æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ä½œä¸ºé»˜è®¤å€¼")
        exp_config = vars(args)
        if args.model_name is None:
            raise ValueError("å¿…é¡»æä¾› --model-name å‚æ•°")
        exp_config['model_name'] = args.model_name
        if args.dataset_name is None:
            raise ValueError("å¿…é¡»æä¾› --dataset-name å‚æ•°")
        exp_config['dataset_name'] = args.dataset_name
        # è®¾ç½®é»˜è®¤çš„windowå’Œstrideï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if 'window' not in exp_config:
            exp_config['window'] = 10
        if 'stride' not in exp_config:
            exp_config['stride'] = 1

    ########################################
    # load dataset                         #
    ########################################

    # è§£æç‰¹å¾åˆ—
    feature_cols = None
    if args.feature_cols:
        feature_cols = [col.strip() for col in args.feature_cols.split(',')]
    
    dataset_name = exp_config.get('dataset_name', args.dataset_name)
    dataset = get_dataset(
        dataset_name,
        data_path=args.data_path,
        static_data_path=args.static_data_path,
        mask_data_path=args.mask_data_path,
        feature_cols=feature_cols
    )

    ########################################
    # load data module                     #
    ########################################

    # time embedding
    u_size = 1  # é»˜è®¤å€¼
    if is_spin or args.model_name == 'transformer':
        # laneæ•°æ®é›†ä½¿ç”¨ç©ºçš„æ—¶é—´ç¼–ç åˆ—è¡¨
        if dataset_name == 'lane':
            time_emb = dataset.datetime_encoded([]).values
        else:
            time_emb = dataset.datetime_encoded(['day', 'week']).values
        # è·å–æ—¶é—´ç¼–ç çš„å®é™…ç»´åº¦
        u_size = time_emb.shape[-1] if len(time_emb.shape) > 1 else 1
        exog_map = {'global_temporal_encoding': time_emb}

        input_map = {
            'u': 'temporal_encoding',
            'x': 'data'
        }
    else:
        exog_map = input_map = None

    if is_spin or args.model_name == 'grin':
        adj = dataset.get_connectivity(threshold=args.adj_threshold,
                                       include_self=False,
                                       force_symmetric=is_spin)
        # å°†é‚»æ¥çŸ©é˜µè½¬æ¢ä¸º edge_index æ ¼å¼ (2, num_edges)
        from tsl.ops.connectivity import adj_to_edge_index
        edge_index, edge_weight = adj_to_edge_index(adj)
        connectivity = (edge_index, edge_weight)
    else:
        connectivity = None

    # instantiate dataset
    data, index, node_ids = dataset.numpy(return_idx=True)
    torch_dataset = ImputationDataset(data=data,
                                      index=index,
                                      training_mask=dataset.training_mask,
                                      eval_mask=dataset.eval_mask,
                                      connectivity=connectivity,
                                      exogenous=exog_map,
                                      input_map=input_map,
                                      window=exp_config.get('window', 10),
                                      stride=exp_config.get('stride', 1))
    
    # å¦‚æœæ•°æ®é›†æœ‰æ–‡ä»¶è¾¹ç•Œä¿¡æ¯ï¼Œè¿‡æ»¤è·¨è¶Šè¾¹ç•Œçš„çª—å£
    if hasattr(dataset, 'file_boundaries') and dataset.file_boundaries:
        print(f"\nğŸ” æ£€æµ‹åˆ° {len(dataset.file_boundaries)} ä¸ªæ–‡ä»¶è¾¹ç•Œï¼Œå¼€å§‹è¿‡æ»¤è·¨è¶Šè¾¹ç•Œçš„çª—å£...")
        torch_dataset = filter_cross_boundary_windows(
            torch_dataset, 
            dataset.file_boundaries, 
            exp_config.get('window', 10)
        )

    # get train/val/test indices
    splitter = dataset.get_splitter(args.val_len, args.test_len)

    scalers = {'data': StandardScaler(axis=(0, 1))}

    dm = SpatioTemporalDataModule(torch_dataset,
                                  scalers=scalers,
                                  splitter=splitter,
                                  batch_size=args.batch_size)
    dm.setup()

    ########################################
    # load model                           #
    ########################################

    imputer = load_model(exp_dir, exp_config, dm, 
                        checkpoint_path=args.checkpoint_path,
                        u_size=u_size)

    trainer = pl.Trainer(accelerator='gpu' if torch.cuda.is_available() else 'cpu',
                        devices=1 if torch.cuda.is_available() else None)

    ########################################
    # inference                            #
    ########################################

    seeds = ensure_list(args.test_mask_seed) if args.test_mask_seed is not None else [None]
    mae = []

    for seed in seeds:
        # Change evaluation mask (ä»…å¯¹élaneæ•°æ®é›†ï¼Œlaneæ•°æ®é›†ä½¿ç”¨è‡ªå·±çš„mask)
        if dataset_name != 'lane':
            update_test_eval_mask(dm, dataset, args.p_fault, args.p_noise, seed)

        output_list = trainer.predict(imputer, dataloaders=dm.test_dataloader())
        
        # å°†å­—å…¸åˆ—è¡¨åˆå¹¶ä¸ºå•ä¸ªå­—å…¸ï¼Œæ¯ä¸ªé”®åŒ…å«æ‰€æœ‰æ‰¹æ¬¡çš„æ‹¼æ¥ç»“æœ
        # output_list æ˜¯ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å« 'y_hat', 'y', 'mask'
        y_hat_list = []
        y_list = []
        mask_list = []
        
        for batch_output in output_list:
            y_hat_list.append(batch_output['y_hat'].detach().cpu())
            y_list.append(batch_output['y'].detach().cpu())
            mask_list.append(batch_output['mask'].detach().cpu())
        
        # æ‹¼æ¥æ‰€æœ‰æ‰¹æ¬¡
        y_hat = torch.cat(y_hat_list, dim=0).numpy()
        y_true = torch.cat(y_list, dim=0).numpy()
        mask = torch.cat(mask_list, dim=0).numpy()
        
        # åªåœ¨æœ€åä¸€ä¸ªç»´åº¦å¤§å°ä¸º1æ—¶æ‰squeeze
        if y_hat.shape[-1] == 1:
            y_hat = y_hat.squeeze(-1)
        if y_true.shape[-1] == 1:
            y_true = y_true.squeeze(-1)
        if mask.shape[-1] == 1:
            mask = mask.squeeze(-1)

        check_mae = numpy_metrics.masked_mae(y_hat, y_true, mask)
        mae.append(check_mae)
        seed_str = f'SEED {seed}' if seed is not None else 'NO SEED'
        print(f'{seed_str} - Test MAE: {check_mae:.3f}')
        
        # ä¿å­˜å¡«å……ç»“æœï¼ˆä»…å¯¹laneæ•°æ®é›†ï¼Œä¸”åªä¿å­˜ç¬¬ä¸€ä¸ªseedçš„ç»“æœï¼‰
        if dataset_name == 'lane' and args.output_path is not None and seed == seeds[0]:
            print(f"\nä¿å­˜å¡«å……ç»“æœ...")
            # å¯¹æ‰€æœ‰æ•°æ®è¿›è¡Œæ¨ç†ï¼ˆä¸ä»…ä»…æ˜¯æµ‹è¯•é›†ï¼‰
            print("å¯¹æ‰€æœ‰æ•°æ®è¿›è¡Œæ¨ç†...")
            all_output_list = []
            
            # è®­ç»ƒé›†
            train_dl = dm.train_dataloader()
            if train_dl is not None:
                train_output = trainer.predict(imputer, dataloaders=train_dl)
                if train_output is not None:
                    all_output_list.extend(train_output)
            
            # éªŒè¯é›†
            val_dl = dm.val_dataloader()
            if val_dl is not None:
                val_output = trainer.predict(imputer, dataloaders=val_dl)
                if val_output is not None:
                    all_output_list.extend(val_output)
            
            # æµ‹è¯•é›†
            test_dl = dm.test_dataloader()
            if test_dl is not None:
                test_output = trainer.predict(imputer, dataloaders=test_dl)
                if test_output is not None:
                    all_output_list.extend(test_output)
            
            # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„é¢„æµ‹ç»“æœ
            all_y_hat_list = []
            for batch_output in all_output_list:
                all_y_hat_list.append(batch_output['y_hat'].detach().cpu())
            
            # æ‹¼æ¥æ‰€æœ‰æ‰¹æ¬¡
            all_y_hat = torch.cat(all_y_hat_list, dim=0).numpy()
            
            # åªåœ¨æœ€åä¸€ä¸ªç»´åº¦å¤§å°ä¸º1æ—¶æ‰squeeze
            if all_y_hat.shape[-1] == 1:
                all_y_hat = all_y_hat.squeeze(-1)
            
            # è·å–æ‰€æœ‰ç´¢å¼•
            train_idx, val_idx, test_idx = splitter.split(dataset)
            save_imputed_results_lane(all_y_hat, dataset, dm, args.output_path, 
                                     train_indices=train_idx, val_indices=val_idx, test_indices=test_idx,
                                     mask_data_path=args.mask_data_path)

    if len(mae) > 1:
        print(f'MAE over {len(seeds)} runs: {np.mean(mae):.3f}Â±{np.std(mae):.3f}')
    else:
        print(f'Test MAE: {mae[0]:.3f}')


if __name__ == '__main__':
    args = parse_args()
    run_experiment(args)
