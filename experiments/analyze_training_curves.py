"""
åˆ†æè®­ç»ƒæ›²çº¿ï¼Œåˆ¤æ–­æ¨¡å‹æ˜¯æ¬ æ‹Ÿåˆè¿˜æ˜¯è¿‡æ‹Ÿåˆ

ä½¿ç”¨æ–¹æ³•:
    python experiments/analyze_training_curves.py --logdir <è®­ç»ƒæ—¥å¿—ç›®å½•>
    
æˆ–è€…ç›´æ¥æŒ‡å®šå®éªŒç›®å½•:
    python experiments/analyze_training_curves.py --logdir runs/lane/spin/20240101T120000_12345
"""

import os
import sys
import argparse
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from tensorboard.backend.event_processing.event_accumulator import EventAccumulator
    HAS_TENSORBOARD = True
except ImportError:
    HAS_TENSORBOARD = False
    print("è­¦å‘Š: æœªå®‰è£…tensorboardï¼Œå°†å°è¯•ä½¿ç”¨å…¶ä»–æ–¹æ³•è¯»å–æ—¥å¿—")


def load_tensorboard_logs(logdir):
    """ä»TensorBoardæ—¥å¿—ç›®å½•åŠ è½½è®­ç»ƒæŒ‡æ ‡"""
    if not HAS_TENSORBOARD:
        raise ImportError("éœ€è¦å®‰è£…tensorboard: pip install tensorboard")
    
    # æŸ¥æ‰¾eventsæ–‡ä»¶
    event_files = list(Path(logdir).rglob('events.out.tfevents.*'))
    if not event_files:
        raise FileNotFoundError(f"åœ¨ {logdir} ä¸­æœªæ‰¾åˆ°TensorBoardäº‹ä»¶æ–‡ä»¶")
    
    # ä½¿ç”¨æœ€æ–°çš„eventsæ–‡ä»¶
    event_file = max(event_files, key=lambda x: x.stat().st_mtime)
    event_dir = str(event_file.parent)
    
    print(f"è¯»å–æ—¥å¿—æ–‡ä»¶: {event_file}")
    
    # åŠ è½½äº‹ä»¶
    ea = EventAccumulator(event_dir)
    ea.Reload()
    
    # è·å–æ‰€æœ‰æ ‡é‡æ ‡ç­¾
    scalar_tags = ea.Tags()['scalars']
    print(f"æ‰¾åˆ°çš„æŒ‡æ ‡: {scalar_tags}")
    
    # æå–è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡
    metrics = {}
    for tag in scalar_tags:
        scalar_events = ea.Scalars(tag)
        steps = [s.step for s in scalar_events]
        values = [s.value for s in scalar_events]
        metrics[tag] = {'steps': steps, 'values': values}
    
    return metrics


def _get_metric_values(metrics, metric_name):
    """å°è¯•å¤šç§æ ¼å¼è·å–æŒ‡æ ‡å€¼"""
    # å°è¯•å¤šç§å¯èƒ½çš„é”®åæ ¼å¼
    possible_keys = [
        f'{metric_name}/epoch',  # å¸¦ /epoch åç¼€
        metric_name,             # ä¸å¸¦åç¼€
        f'{metric_name}/step',   # å¸¦ /step åç¼€
    ]
    
    for key in possible_keys:
        if key in metrics:
            return metrics[key].get('values', [])
    
    return []


def analyze_fitting(metrics):
    """åˆ†æè®­ç»ƒæ›²çº¿ï¼Œåˆ¤æ–­æ¬ æ‹Ÿåˆ/è¿‡æ‹Ÿåˆ"""
    results = {
        'status': 'unknown',
        'train_mae': None,
        'val_mae': None,
        'train_loss': None,
        'val_loss': None,
        'gap': None,
        'convergence': None,
        'recommendations': []
    }
    
    # æå–å…³é”®æŒ‡æ ‡ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
    train_mae = _get_metric_values(metrics, 'train_mae')
    val_mae = _get_metric_values(metrics, 'val_mae')
    train_loss = _get_metric_values(metrics, 'train_loss')
    val_loss = _get_metric_values(metrics, 'val_loss')
    
    if not train_mae or not val_mae:
        print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°è¶³å¤Ÿçš„è®­ç»ƒæŒ‡æ ‡æ•°æ®")
        print(f"   æ‰¾åˆ°çš„è®­ç»ƒæŒ‡æ ‡é”®: {list(metrics.keys())}")
        print(f"   train_mae æ•°æ®ç‚¹æ•°: {len(train_mae)}")
        print(f"   val_mae æ•°æ®ç‚¹æ•°: {len(val_mae)}")
        return results
    
    # è·å–æœ€åå‡ ä¸ªepochçš„å¹³å‡å€¼ï¼ˆé¿å…æ³¢åŠ¨ï¼‰
    n_epochs = min(len(train_mae), len(val_mae))
    if n_epochs < 5:
        print("âš ï¸  è­¦å‘Š: è®­ç»ƒè½®æ•°å¤ªå°‘ï¼Œæ— æ³•å‡†ç¡®åˆ¤æ–­")
        return results
    
    # è®¡ç®—æœ€å5ä¸ªepochçš„å¹³å‡å€¼
    last_n = min(5, n_epochs)
    final_train_mae = np.mean(train_mae[-last_n:])
    final_val_mae = np.mean(val_mae[-last_n:])
    final_train_loss = np.mean(train_loss[-last_n:]) if train_loss else None
    final_val_loss = np.mean(val_loss[-last_n:]) if val_loss else None
    
    results['train_mae'] = final_train_mae
    results['val_mae'] = final_val_mae
    results['train_loss'] = final_train_loss
    results['val_loss'] = final_val_loss
    
    # è®¡ç®—gapï¼ˆéªŒè¯é›†å’Œè®­ç»ƒé›†çš„å·®å¼‚ï¼‰
    mae_gap = final_val_mae - final_train_mae
    loss_gap = (final_val_loss - final_train_loss) if (final_val_loss and final_train_loss) else None
    
    results['gap'] = mae_gap
    
    # åˆ¤æ–­æ”¶æ•›æƒ…å†µ
    if n_epochs >= 10:
        # æ£€æŸ¥æœ€å10ä¸ªepochæ˜¯å¦è¿˜åœ¨ä¸‹é™
        recent_train = train_mae[-10:]
        recent_val = val_mae[-10:]
        train_trend = np.polyfit(range(len(recent_train)), recent_train, 1)[0]
        val_trend = np.polyfit(range(len(recent_val)), recent_val, 1)[0]
        
        if train_trend < -0.01 and val_trend < -0.01:
            results['convergence'] = 'still_improving'
        elif abs(train_trend) < 0.01 and abs(val_trend) < 0.01:
            results['convergence'] = 'converged'
        else:
            results['convergence'] = 'fluctuating'
    else:
        results['convergence'] = 'unknown'
    
    # åˆ¤æ–­æ¬ æ‹Ÿåˆ/è¿‡æ‹Ÿåˆ
    gap_ratio = mae_gap / final_train_mae if final_train_mae > 0 else 0
    
    if final_train_mae > final_val_mae * 1.1:
        # è®­ç»ƒé›†MAEæ˜æ˜¾é«˜äºéªŒè¯é›†ï¼ˆå¼‚å¸¸æƒ…å†µï¼Œå¯èƒ½æ˜¯æ•°æ®é—®é¢˜ï¼‰
        results['status'] = 'anomaly'
        results['recommendations'].append("è®­ç»ƒé›†MAEé«˜äºéªŒè¯é›†ï¼Œå¯èƒ½æ˜¯æ•°æ®åˆ’åˆ†æˆ–maskè®¾ç½®æœ‰é—®é¢˜")
    elif gap_ratio > 0.3:
        # Gap > 30%ï¼Œè¿‡æ‹Ÿåˆ
        results['status'] = 'overfitting'
        results['recommendations'].extend([
            "å¢åŠ æ­£åˆ™åŒ–: l2_reg = 1e-4 æˆ– 1e-3",
            "å¢åŠ dropoutï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰",
            "å‡å°‘æ¨¡å‹å®¹é‡: hidden_size æˆ– n_layers",
            "å¢åŠ æ•°æ®å¢å¼ºæˆ–whiten_prob",
            "ä½¿ç”¨æ—©åœï¼ˆpatienceï¼‰"
        ])
    elif gap_ratio < 0.05 and final_train_mae > 0.5:
        # Gapå¾ˆå°ä½†MAEä»ç„¶å¾ˆé«˜ï¼Œæ¬ æ‹Ÿåˆ
        results['status'] = 'underfitting'
        results['recommendations'].extend([
            "å¢åŠ æ¨¡å‹å®¹é‡: hidden_size (16â†’32â†’64)",
            "å¢åŠ å±‚æ•°: n_layers (3â†’4â†’5)",
            "å¢åŠ è®­ç»ƒè½®æ•°: epochs",
            "å¢åŠ å­¦ä¹ ç‡: lr (0.0008â†’0.001â†’0.0015)",
            "æ£€æŸ¥æ•°æ®é¢„å¤„ç†æ˜¯å¦æ­£ç¡®"
        ])
    elif gap_ratio < 0.05:
        # Gapå°ä¸”MAEè¾ƒä½ï¼Œå¯èƒ½æ˜¯è‰¯å¥½æ‹Ÿåˆ
        results['status'] = 'good_fit'
        results['recommendations'].append("æ¨¡å‹æ‹Ÿåˆè‰¯å¥½ï¼Œå¯ä»¥å°è¯•å¾®è°ƒè¶…å‚æ•°è¿›ä¸€æ­¥æå‡")
    elif 0.05 <= gap_ratio <= 0.3:
        # ä¸­ç­‰gapï¼Œå¯èƒ½æ˜¯è½»å¾®è¿‡æ‹Ÿåˆæˆ–æ­£å¸¸
        results['status'] = 'slight_overfitting'
        results['recommendations'].extend([
            "è½»å¾®è¿‡æ‹Ÿåˆï¼Œå¯ä»¥å¢åŠ å°‘é‡æ­£åˆ™åŒ–",
            "æˆ–ç»§ç»­è®­ç»ƒè§‚å¯Ÿæ˜¯å¦æ”¹å–„"
        ])
    else:
        results['status'] = 'unknown'
    
    return results


def _get_metric_data(metrics, metric_name):
    """è·å–æŒ‡æ ‡çš„æ•°æ®ï¼ˆstepså’Œvaluesï¼‰"""
    possible_keys = [
        f'{metric_name}/epoch',
        metric_name,
        f'{metric_name}/step',
    ]
    
    for key in possible_keys:
        if key in metrics:
            return metrics[key].get('steps', []), metrics[key].get('values', [])
    
    return None, None


def plot_training_curves(metrics, output_path=None):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Training Curves Analysis', fontsize=16, fontweight='bold')
    
    # MAEæ›²çº¿
    ax1 = axes[0, 0]
    train_steps, train_values = _get_metric_data(metrics, 'train_mae')
    if train_steps and train_values:
        ax1.plot(train_steps, train_values, label='Train MAE', linewidth=2, alpha=0.8)
    
    val_steps, val_values = _get_metric_data(metrics, 'val_mae')
    if val_steps and val_values:
        ax1.plot(val_steps, val_values, label='Val MAE', linewidth=2, alpha=0.8)
    
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('MAE')
    ax1.set_title('Mean Absolute Error')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Lossæ›²çº¿
    ax2 = axes[0, 1]
    train_loss_steps, train_loss_values = _get_metric_data(metrics, 'train_loss')
    if train_loss_steps and train_loss_values:
        ax2.plot(train_loss_steps, train_loss_values, label='Train Loss', linewidth=2, alpha=0.8)
    
    val_loss_steps, val_loss_values = _get_metric_data(metrics, 'val_loss')
    if val_loss_steps and val_loss_values:
        ax2.plot(val_loss_steps, val_loss_values, label='Val Loss', linewidth=2, alpha=0.8)
    
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Loss')
    ax2.set_title('Loss')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # MSEæ›²çº¿
    ax3 = axes[1, 0]
    train_mse_steps, train_mse_values = _get_metric_data(metrics, 'train_mse')
    if train_mse_steps and train_mse_values:
        ax3.plot(train_mse_steps, train_mse_values, label='Train MSE', linewidth=2, alpha=0.8)
    
    val_mse_steps, val_mse_values = _get_metric_data(metrics, 'val_mse')
    if val_mse_steps and val_mse_values:
        ax3.plot(val_mse_steps, val_mse_values, label='Val MSE', linewidth=2, alpha=0.8)
    
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('MSE')
    ax3.set_title('Mean Squared Error')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # Gapæ›²çº¿ï¼ˆéªŒè¯é›† - è®­ç»ƒé›†ï¼‰
    ax4 = axes[1, 1]
    train_mae_steps, train_mae_values = _get_metric_data(metrics, 'train_mae')
    val_mae_steps, val_mae_values = _get_metric_data(metrics, 'val_mae')
    
    if train_mae_steps and train_mae_values and val_mae_steps and val_mae_values:
        # å¯¹é½steps
        common_steps = sorted(set(train_mae_steps) & set(val_mae_steps))
        if common_steps:
            train_aligned = [train_mae_values[train_mae_steps.index(s)] for s in common_steps]
            val_aligned = [val_mae_values[val_mae_steps.index(s)] for s in common_steps]
            gap = [v - t for t, v in zip(train_aligned, val_aligned)]
            
            ax4.plot(common_steps, gap, label='Val - Train Gap', linewidth=2, 
                    color='red', alpha=0.8)
            ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)
            ax4.fill_between(common_steps, 0, gap, alpha=0.3, color='red')
    
    ax4.set_xlabel('Epoch')
    ax4.set_ylabel('Gap (Val - Train)')
    ax4.set_title('Overfitting Indicator (Gap)')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"å›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")
    else:
        plt.show()


def print_analysis_report(results, metrics):
    """æ‰“å°åˆ†ææŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“Š è®­ç»ƒæ›²çº¿åˆ†ææŠ¥å‘Š")
    print("="*60)
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"\nğŸ¯ æ¨¡å‹çŠ¶æ€: {results['status']}")
    print(f"\nğŸ“ˆ å…³é”®æŒ‡æ ‡ (æœ€å5ä¸ªepochçš„å¹³å‡å€¼):")
    if results['train_mae'] is not None:
        print(f"   è®­ç»ƒé›† MAE: {results['train_mae']:.4f}")
    if results['val_mae'] is not None:
        print(f"   éªŒè¯é›† MAE: {results['val_mae']:.4f}")
    if results['gap'] is not None:
        print(f"   Gap (Val - Train): {results['gap']:.4f} ({results['gap']/results['train_mae']*100:.1f}%)")
    
    if results['convergence']:
        conv_map = {
            'still_improving': 'ä»åœ¨æ”¹å–„',
            'converged': 'å·²æ”¶æ•›',
            'fluctuating': 'æ³¢åŠ¨ä¸­',
            'unknown': 'æœªçŸ¥'
        }
        print(f"\nğŸ”„ æ”¶æ•›çŠ¶æ€: {conv_map.get(results['convergence'], results['convergence'])}")
    
    # åˆ¤æ–­ç»“æœ
    status_map = {
        'overfitting': 'ğŸ”´ è¿‡æ‹Ÿåˆ',
        'underfitting': 'ğŸŸ¡ æ¬ æ‹Ÿåˆ',
        'good_fit': 'ğŸŸ¢ è‰¯å¥½æ‹Ÿåˆ',
        'slight_overfitting': 'ğŸŸ  è½»å¾®è¿‡æ‹Ÿåˆ',
        'anomaly': 'âš ï¸  å¼‚å¸¸',
        'unknown': 'â“ æœªçŸ¥'
    }
    
    print(f"\n{status_map.get(results['status'], results['status'])}")
    
    # å»ºè®®
    if results['recommendations']:
        print(f"\nğŸ’¡ è°ƒæ•´å»ºè®®:")
        for i, rec in enumerate(results['recommendations'], 1):
            print(f"   {i}. {rec}")
    
    print("\n" + "="*60)


def main():
    parser = argparse.ArgumentParser(description='åˆ†æè®­ç»ƒæ›²çº¿åˆ¤æ–­æ¬ æ‹Ÿåˆ/è¿‡æ‹Ÿåˆ')
    parser.add_argument('--logdir', type=str, required=True,
                       help='è®­ç»ƒæ—¥å¿—ç›®å½•è·¯å¾„ (ä¾‹å¦‚: runs/lane/spin/20240101T120000_12345)')
    parser.add_argument('--save-plot', type=str, default=None,
                       help='ä¿å­˜å›¾è¡¨è·¯å¾„ (ä¾‹å¦‚: training_curves.png)')
    parser.add_argument('--show-plot', action='store_true',
                       help='æ˜¾ç¤ºå›¾è¡¨')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ç›®å½•
    if not os.path.exists(args.logdir):
        print(f"âŒ é”™è¯¯: ç›®å½•ä¸å­˜åœ¨: {args.logdir}")
        return
    
    # åŠ è½½æŒ‡æ ‡
    try:
        metrics = load_tensorboard_logs(args.logdir)
    except Exception as e:
        print(f"âŒ é”™è¯¯: æ— æ³•åŠ è½½æ—¥å¿—: {e}")
        return
    
    # åˆ†æ
    results = analyze_fitting(metrics)
    
    # æ‰“å°æŠ¥å‘Š
    print_analysis_report(results, metrics)
    
    # ç»˜åˆ¶æ›²çº¿
    if args.show_plot or args.save_plot:
        try:
            plot_training_curves(metrics, output_path=args.save_plot)
            if args.show_plot:
                plt.show()
        except Exception as e:
            print(f"âš ï¸  è­¦å‘Š: æ— æ³•ç»˜åˆ¶å›¾è¡¨: {e}")


if __name__ == '__main__':
    main()


